{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dc-neo/cyber_security/blob/main/Sentiment_Analysis_Supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcAtEF6X7-yJ"
      },
      "source": [
        "# Import necessary depencencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cVFxDEtK7-yN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import text_normalizer as tn\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "24ifs3y_8ZsB",
        "outputId": "b7971cc3-7f60-4ffe-8c1f-4c2f99e93bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')\n",
        "os.getcwd()\n"
      ],
      "metadata": {
        "id": "gFgZWZ7L8Vde",
        "outputId": "392c132f-0abb-42c7-9f2b-5aa245be23da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import model_evaluation_utils as meu"
      ],
      "metadata": {
        "id": "ufKn7vrj9pH3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p17_U9E7-yO"
      },
      "source": [
        "# Load and normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y6mTsGoK7-yP",
        "outputId": "b22bc2fe-8128-4ca8-ad64-11ef9f54750d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  not bother think would see movie great supspen...  negative\n",
            "1  careful one get mitt change way look kung fu f...  positive\n",
            "2  chili palmer tired movie know want success mus...  negative\n",
            "3  follow little know 1998 british film make budg...  positive\n",
            "4  dark angel cross huxley brave new world percys...  positive\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(r'movie_reviews_cleaned.csv')\n",
        "\n",
        "# take a peek at the data\n",
        "print(dataset.head())\n",
        "reviews = np.array(dataset['review'])\n",
        "sentiments = np.array(dataset['sentiment'])\n",
        "\n",
        "# build train and test datasets\n",
        "train_reviews = reviews[:5000]\n",
        "train_sentiments = sentiments[:5000]\n",
        "test_reviews = reviews[5000:7000]\n",
        "test_sentiments = sentiments[5000:7000]\n",
        "\n",
        "# normalize datasets\n",
        "norm_train_reviews = train_reviews\n",
        "norm_test_reviews = test_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKadjE-a7-yQ"
      },
      "source": [
        "# Traditional Supervised Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOA4C_5O7-yQ"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "QR1ZPbhZ7-yR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "ZmOs57Df7-yR"
      },
      "outputs": [],
      "source": [
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H_fPvkmy7-yR",
        "outputId": "c1f85bb0-15d1-4ce1-d0d8-a3e9147e4ebd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW model:> Train features shape: (5000, 434563)  Test features shape: (2000, 434563)\n",
            "TFIDF model:> Train features shape: (5000, 434563)  Test features shape: (2000, 434563)\n"
          ]
        }
      ],
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luk2F_zC7-yS"
      },
      "source": [
        "## Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "VN9Y2PW37-yS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
        "svm = SGDClassifier(loss='hinge', max_iter=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iCC36HXX7-yT",
        "outputId": "c5b150f5-5c58-45c7-8e16-6549457abe21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8605\n",
            "Precision: 0.8606\n",
            "Recall: 0.8605\n",
            "F1 Score: 0.8605\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.85      0.86      0.86       981\n",
            "    negative       0.87      0.86      0.86      1019\n",
            "\n",
            "    accuracy                           0.86      2000\n",
            "   macro avg       0.86      0.86      0.86      2000\n",
            "weighted avg       0.86      0.86      0.86      2000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive        846      135\n",
            "        negative        144      875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression model on BOW features\n",
        "lr_bow_predictions = meu.train_predict_model(classifier=lr,\n",
        "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
        "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "X1SGZqT47-yT",
        "outputId": "ca1f025c-4f9c-494d-d609-13b91a1955e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.866\n",
            "Precision: 0.8661\n",
            "Recall: 0.866\n",
            "F1 Score: 0.866\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.87      0.85      0.86       981\n",
            "    negative       0.86      0.88      0.87      1019\n",
            "\n",
            "    accuracy                           0.87      2000\n",
            "   macro avg       0.87      0.87      0.87      2000\n",
            "weighted avg       0.87      0.87      0.87      2000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive        838      143\n",
            "        negative        125      894\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression model on TF-IDF features\n",
        "lr_tfidf_predictions = meu.train_predict_model(classifier=lr,\n",
        "                                               train_features=tv_train_features, train_labels=train_sentiments,\n",
        "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SzQJdrPw7-yU",
        "outputId": "1360ad29-5312-4343-91bf-2ed33248548f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.85\n",
            "Precision: 0.8502\n",
            "Recall: 0.85\n",
            "F1 Score: 0.8499\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.86      0.83      0.84       981\n",
            "    negative       0.84      0.87      0.85      1019\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive        817      164\n",
            "        negative        136      883\n"
          ]
        }
      ],
      "source": [
        "svm_bow_predictions = meu.train_predict_model(classifier=svm,\n",
        "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
        "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N1YiTI_H7-yU",
        "outputId": "83570a05-ba86-4074-ef94-65dfe21a12d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.88\n",
            "Precision: 0.88\n",
            "Recall: 0.88\n",
            "F1 Score: 0.88\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.87      0.88       981\n",
            "    negative       0.88      0.89      0.88      1019\n",
            "\n",
            "    accuracy                           0.88      2000\n",
            "   macro avg       0.88      0.88      0.88      2000\n",
            "weighted avg       0.88      0.88      0.88      2000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive        857      124\n",
            "        negative        116      903\n"
          ]
        }
      ],
      "source": [
        "svm_tfidf_predictions = meu.train_predict_model(classifier=svm,\n",
        "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
        "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDCIpn1D7-yV"
      },
      "source": [
        "# Newer Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MdQhUvMz7-yV"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ei4e1Y9A9AZ",
        "outputId": "c0e9ca54-69bf-4052-afd5-d2d8b7543276"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 19:36:36.311452: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:36:36.311523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:36:36.311562: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:36:39.035375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2yzM0i97-yV"
      },
      "source": [
        "## Prediction class label encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DPs9VkdE7-yV"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "num_classes=2\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [tokenizer.tokenize(text)\n",
        "                   for text in norm_train_reviews]\n",
        "y_tr = le.fit_transform(train_sentiments)\n",
        "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [tokenizer.tokenize(text)\n",
        "                   for text in norm_test_reviews]\n",
        "y_ts = le.fit_transform(test_sentiments)\n",
        "y_test = keras.utils.to_categorical(y_ts, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9_1HRcTP7-yW",
        "outputId": "e6a68498-b761-48c5-c93d-bd51f22edc5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
            "Sample test label transformation:\n",
            "----------------------------------- \n",
            "Actual Labels: ['negative' 'negative' 'negative'] \n",
            "Encoded Labels: [0 0 0] \n",
            "One hot encoded Labels:\n",
            " [[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3],\n",
        "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxaJN8RK7-yW"
      },
      "source": [
        "## Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j_XO6jb-7-yW"
      },
      "outputs": [],
      "source": [
        "# build word2vec model\n",
        "w2v_num_features = 500\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, vector_size=w2v_num_features, window=150,\n",
        "                                   min_count=10, sample=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "ub9Bnp0N7-yX"
      },
      "outputs": [],
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index_to_key)\n",
        "\n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "kyx3hWsA7-yX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "d7814c23-4518-4663-da4f-858f6561c5bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-44bea4ea1666>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate averaged word vector features from word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                      num_features=500)\n\u001b[1;32m      4\u001b[0m avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n\u001b[1;32m      5\u001b[0m                                                     num_features=500)\n",
            "\u001b[0;32m<ipython-input-24-2e3ba1dbcce1>\u001b[0m in \u001b[0;36maveraged_word2vec_vectorizer\u001b[0;34m(corpus, model, num_features)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[0m\u001b[1;32m     18\u001b[0m                     for tokenized_sentence in corpus]\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2e3ba1dbcce1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[0m\u001b[1;32m     18\u001b[0m                     for tokenized_sentence in corpus]\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2e3ba1dbcce1>\u001b[0m in \u001b[0;36maverage_word_vectors\u001b[0;34m(words, model, vocabulary, num_features)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mnwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=500)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AmQLGtk77-yX"
      },
      "outputs": [],
      "source": [
        "# feature engineering with GloVe model\n",
        "train_nlp = [tn.nlp(item) for item in norm_train_reviews]\n",
        "train_glove_features = np.array([item.vector for item in train_nlp])\n",
        "\n",
        "test_nlp = [tn.nlp(item) for item in norm_test_reviews]\n",
        "test_glove_features = np.array([item.vector for item in test_nlp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VqX7Mu07-yX"
      },
      "outputs": [],
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
        "print('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D9oSJJ77-yY"
      },
      "source": [
        "## Modeling with deep neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-omDWjt7-yY"
      },
      "source": [
        "### Building Deep neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HncJkC2w7-yY"
      },
      "outputs": [],
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    dnn_model.add(Dense(512, activation='relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    dnn_model.add(Dense(512, activation='relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    dnn_model.add(Dense(2))\n",
        "    dnn_model.add(Activation('softmax'))\n",
        "\n",
        "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "De0MGMuh7-yY"
      },
      "outputs": [],
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_R7G2t7-yZ"
      },
      "source": [
        "### Visualize sample deep architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kkepyyt7-yZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False,\n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8odw7iN87-ya"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppOz3ycU7-ya"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=5, batch_size=batch_size,\n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44NuxKUO7-yb"
      },
      "outputs": [],
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "predictions = le.inverse_transform(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH5edcR87-yc"
      },
      "outputs": [],
      "source": [
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZYmvgdwc7-yc"
      },
      "outputs": [],
      "source": [
        "glove_dnn = construct_deepnn_architecture(num_input_features=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mREjCp2l7-yc"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "glove_dnn.fit(train_glove_features, y_train, epochs=5, batch_size=batch_size,\n",
        "              shuffle=True, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7ZIdJUJ7-yd"
      },
      "outputs": [],
      "source": [
        "y_pred = glove_dnn.predict_classes(test_glove_features)\n",
        "predictions = le.inverse_transform(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg4zsT7X7-ye"
      },
      "outputs": [],
      "source": [
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}